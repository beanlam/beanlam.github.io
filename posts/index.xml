<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on beanlam</title>
    <link>/posts/</link>
    <description>Recent content in Posts on beanlam</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>cn</language>
    <lastBuildDate>Thu, 21 Mar 2019 17:36:44 +0800</lastBuildDate>
    
	<atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Java 各版本的特性（更新到 Java 12）</title>
      <link>/2019/java-features/</link>
      <pubDate>Thu, 21 Mar 2019 17:36:44 +0800</pubDate>
      
      <guid>/2019/java-features/</guid>
      <description>JDK 1.1 (1997 年 2 月 19 号)  引入 JDBC (Java Database Connectivity)； 引入内部类 (Inner Classes)； 引入 Java Beans； 引入 RMI (Remote Method Invocation)； 添加只支持内省(Introspection)，但不允许在运行时改动的Java反射机制； 对 AWT(java.awt) 事件模型进行大范围的改进； 支持 Internationalization 和 Unicode；  J2SE 1.2 (1998 年 12 月 4 号)  引入 Java 插件； 引入集合(Collection)框架； 引入 JIT(Just In Time)编译器； 引入 JFC(Java Foundation Classes)，包括 Swing 1.0、拖放和 Java 2D 类库； 引入对打包的 Java 文件进行数字签名； 引入控制授权访问系统资源的策略工具； 对字符串常量做内存映射； 在 Applet 中添加声音支持； 在 JDBC 中引入可滚动结果集、BLOB、CLOB、批量更新和用户自定义类型； 新增关键字 strictfp(strict float point)； 添加可与 CORBA 协同交互的 Java IDL；  J2SE 1.</description>
    </item>
    
    <item>
      <title>关于时间的想法</title>
      <link>/2019/about-time/</link>
      <pubDate>Thu, 07 Mar 2019 11:49:40 +0800</pubDate>
      
      <guid>/2019/about-time/</guid>
      <description>工作时间与自我时间 8小时工作制，是从工业革命一直保留下来的一个传统，一直到现在的21世纪。
大多数人将“工作时间”和“自我时间”划了明确的界限，潜意识里，很多人都认为除了工作的时间，“自我时间“应该是轻松的、放松的，可以放空自己，让工作的烦恼暂时远离自己。
但正如“自我时间”这个词的表面意思所暗示的，自我的时间，其实是关注自己的时间，关注自己的目标的时间。如果你的目标是在未来的某一天，成为一个企业家，那么你应该把“自我时间”投资在如何成为一个企业家这件事上，在你的“自我时间”里，如果你认为你需要娱乐，你需要休息，那么就去娱乐，就去休息。但把“自我时间”定义为绝对的休闲、放松，是不对的。
除了工作时间和睡觉时间，每个人大概有8个小时的自我时间，意味着一年大概有3000个小时的自我时间，如果合理运用这些时间，很多事情都可以在这3000个小时内实现。
复利的威力  大多数人高估了他们一天内能做的事情，却低估了他们一年内能做的事情
 通过复利曲线，感受一下复利的威力
 1.01³⁶⁵ = 37.8 1.10³⁰ = 17.5  每天持续改进1%，坚持一年，跟每天改进10%，但只持续一个月，收效是不同的。 所以仅仅知道复利的威力是不够的，复利的核心是“持续性”，只有持续，长久地做一个事情，才会收获复利带来的效果。
事情优先级  如果每件事的优先级一样，那么等同于没有优先级
 必须为事情定义一个优先级，事情的优先级是随着时间的移动而不断动态调整的，并且具有核心优先级的事情的数量不能太多，最好低于三个。
将事情分为不同层次的优先级，层次1为最高，高优先级的事情，应该投入最多的时间。
重新组织思维方式 不要用二分法思考事情，比如说：
 技术与非技术 快乐与难过 员工与老板  一个事情，它既可以是非技术的，也可以是技术的，一个人，它可能是员工，它从某些层面来说也可以是老板。
要用关联的思维去思考事情，能让你看到更多选项，更多可能，并且在做一个事情的过程中，同时可以做另外一个事情，如果另外一个事情更吸引你，那么可以慢慢过渡到另外一个事情上。而不是非黑即白。
优化生活 注重“元任务”和“绝对任务”，把时间多花在这样的事情上。 元任务，即如果持续地做这件事一年，一年后如果的生活发生了改进，那么就属于元任务。洗了一年的衣服，买了一年的杂货，这些都不能算元任务。坚持一年的写作，一年后写作能力大幅提升，那写作就是一种元任务。如果你坚持了一年的睡前冥想，那么一年后你的心态就会变得更加平和，更能控制自己的情绪。 绝对任务与元任务的相同点是，都对自己的某方面有改进。但绝对任务带来的改进是可以量化的，实际上感受得到的，比如说，坚持了一年的健身改进了身体健康程度，身体线条更优美；坚持一年的阅读改进了阅读速度、增进了知识面。
一夜的美梦 没有人是突然过了一个晚上后就成功的。</description>
    </item>
    
    <item>
      <title>《编程大师访谈录》文摘1</title>
      <link>/2019/master-interview-pick-1/</link>
      <pubDate>Thu, 28 Feb 2019 17:50:09 +0800</pubDate>
      
      <guid>/2019/master-interview-pick-1/</guid>
      <description>查尔斯.西蒙尼 人物介绍 1948年9月10日，查尔斯·西蒙尼（Charles Simonyi）出生于匈牙利布达佩斯。上高中时，他开始接触计算机和编程，父亲安排他给一名从事计算机工作的工程师当助手，当时计算机在匈牙利屈指可数。
1966年，查尔斯高中毕业，同时也完成了他的第一个编译器。凭借开发编译器时积累的经验，他在丹麦哥本哈根的A/S Regnecentralen公司谋得了一个职位。
1968年，他离开丹麦进入美国加州大学伯克利分校学习，并于1972年获得理学学士学位，1977年获得斯坦福大学博士学位。
西蒙尼曾先后在加州大学伯克利分校计算机中心、伯克利计算机公司、ILLIAC 4项目和施乐PA R C工作。自1981年以来，他一直供职于微软公司。在施乐公司，他开发了Alto个人电脑的Bravo和Bravo X程序。在微软，他组建了应用软件小组，并领导开发出Multiplan、Microsoft Word、Microsoft Excel等广受欢迎的应用软件。
编程的步骤 编程的第一步是想象。就是要在脑海中对来龙去脉有极为清晰的把握。在这个初始阶段，我会使用纸和铅笔。我只是信手涂鸦，并不写代码。我也许会画些方框或箭头，但基本上只是涂鸦，因为真正的想法在我脑海里。我喜欢想象那些有待维护的结构，那些结构代表着我想编码的真实世界。一旦这个结构考虑得相当严谨和明确，我便开始写代码。我会坐到终端前，或者换在以前的话，就会拿张白纸，开始写代码。这相当容易。我只要把头脑中的想法变换成代码写下来，我知道结果应该是什么样的。大部分代码会水到渠成，不过我维护的那些数据结构才是关键。我会先想好数据结构，并在整个编码过程中将它们牢记于心。
团队大小 让多名程序员开发一个程序，开发速度会更快吗？ 不一定。编写同一个程序的人员越多，人均产出的实际代码量越少。结果，总的代码产出一开始会更多，之后实际上可能会减少。以两个人为例，也许单位时间只能多写百分之五十的代码。
顺便提一下，代码的效率还会随着开发同一个程序的人员数量的增加而有所降低。最高效的程序往往是一个人写的。唯一的问题是，它可能需要写上一辈子，而这显然是无法接受的。因此你需要找上三五十个，甚或好几百个人开发一个项目
巴特勒.兰普森 人物介绍 巴特勒·兰普森（Butler Lampson），目前在加州帕洛阿尔托数字设备公司（Digital Equipment Corporation，下文简称DEC①公司）系统研究中心担任高级工程师，他曾是加州大学伯克利分校计算机科学副教授、伯克利计算机公司创始人、施乐PA R C计算机科学实验室的高级研究员。
兰普森是业界最受敬重的专家之一，在许多计算机设计和研究领域都颇有建树。他开发过硬件系统，如以太网局域网和Alto、Dorado个人电脑；操作系统，如SDS 940和Alto；编程语言，如LISP和Mesa；应用程序，如Bravo编辑器和Star办公系统；还有网络服务器，如Dover打印机和Grapevine邮件系统。
物理、数学和计算机科学的联系 就物理学和数学而言，一如其他正统的学科，要想有所成就，必须能够清晰地思考。这就是计算机行业的许多成功人士都来自这些领域的原因所在。而现在人们通常一直待在计算机系，要有所成就会更加困难，因为这是一门非常浅显的学科，无法驱使你全力发挥出自己的聪明才智。
通过数学，你学会逻辑推理。你还会学到如何证明，以及怎么处理抽象要素。通过物理学等实验科学或人文学科，你学会如何应用这些抽象在现实中建立联系。
复杂性与简单性 一切都应该尽可能简单。但要做到这一点你必须掌握复杂性。
控制复杂性有一些基本技巧。从根本上，我会分而治之，把事情分解开，并准确描述各个部分应该实现什么功能。这会变成接下来如何行事的纲要。如果你还没想清楚怎么写规格，那表明你不明白具体是怎么回事。接着，你有两种选择：要么退回到你真正理解的另外某个问题上，要么更努力地思考。
此外，系统的描述不应该太庞大。也许你必须从多个较小部分的角度来考虑一个大系统。这有点像解数学题：你写的书可以包含许多有用的提示，但不能直接给出算法
约翰.沃诺克 人物介绍 约翰·沃诺克（John Warnock）出生于1940年，在犹他州长大，就读于犹他大学，并获得了数学学士和硕士学位，以及计算机科学博士学位。 沃诺克担任Adobe公司的CEO，除了Adobe公司，沃诺克还曾担任Ebrary、MongoNet、Knight-Ridder、Netscape等公司董事，目前仍然是Salon媒体集团董事会主席。他亦曾是圣何塞市创新科技馆的主席，目前还在美国电影协会和圣丹斯协会担任理事。
编程的诀窍 不要早作绑定，尽可能推迟决定时间。眼界放宽一些，设计要比你自认为需要的程度更加灵活，因为从长远看你最终会需要这样。快速让某样东西工作起来，然后还能弃之不用。
从小的开始实验而不是大的入手学习。不要一头扎进周期长达两年且中间不出什么成果的开发当中。最好每两个月就要出点成果，这样你才能进行评估、重组和重新开始。
程序员经常在一开始时过度定义他们的方法。他们可能会从一个中心构思着手，从第一天就开始编码。然后他们发现自己陷入重围，每件东西都开始膨胀，因为它们依赖于太多其他因素。应当反其道而行之，如果让过程较为宽松，保持一定的自由度，并在最后阶段加快速度，长远来说，你会做出更好的产品。
加里.基尔戴尔 人物介绍 作为数字化研究所（DRI）的创始人和董事会主席，加里·A. 基尔代尔（Gary A. Kildall）于1972年到1973年间开发了第一个微机操作系统。他把那个系统称为CP/M（控制程序/监控程序）操作系统，后来成为他们公司的第一款产品。此外，他为IBM个人计算机设计了DR Logo编程语言，并开发出微机上最早的高级计算机语言之一，PL/1。
基尔代尔是西雅图人，生于1942年5月19日。他于1972年获得华盛顿大学计算机科学博士学位，随后加入了海军，并且执教于加利福尼亚州蒙特雷的美国海军研究生院，讲授计算机科学。从海军退役后他仍旧在那里继续执教。
基尔代尔于1984年创建了一家名为Activenture的新公司（最近已更名为KnowledgeSet公司），以探索光盘出版业的潜力。Activenture公司于1985年宣布，他们将出版光盘版的《格罗利尔多媒体百科全书》。基尔代尔在担任KnowledgeSet公司总裁的同时，仍旧担任着数字化研究所董事会主席一职。
编程风格 我遵循非常明确的、适合自己的流程，虽然这些流程可能并不适合别人。我会先画数据结构，然后花很长时间思考数据结构。在开始编程前，我会一直思考整个程序的流程。
在确定数据结构之后，我就开始写一些小段的代码，并不断地改善和监测。在编码过程中进行测试可以确保所做的修改是局部的，并且如果有什么问题的话，能够马上发现。整个迭代改进的过程是需要速度的，至少对我来说，一个快速的编辑、执行和调试周期是非常重要的。
编程是否可以练习 嗯，在某种意义上是可以练习的。计算机语言（标识语）的发明家西摩·佩伯特（Seymour Papert）认为孩子们可以通过摆弄齿轮等机械小玩意而变得具有创造性。通过这种玩耍而学习和练习的技能会转移到其他领域。佩伯特的观点正是我童年的经历。我父亲是一名技艺精湛的工匠。我常常待在旁边看他干活，一看就是好几个小时，然后跑到外面，拿起锤子和钉子模仿他。
作为编程基础的数据结构，从本质上看是机械的，就像我儿时玩的东西。所以，在这个意义上讲，我是可以练习编程的。最大的不同是，木制或铁制的东西需要花好几个小时来建造，如果做得不正确，必须回去重新做。而程序可以在瞬间完成修改。
程序员积累本领的方式 你需要学习其他人的工作。他们解决问题的方法和他们使用的工具会让你以一个新的视角来审视自己的工作。在写程序前只需要学习一组为数不多的程序模块。例如，要写编译器，首先要写的是扫描功能，那是一个会用到很多次的小工具。一旦学会了这些工具，剩下的工作就只是把它们组合在一起。这里弄点儿、那里弄点儿，把这些功能模块都放到一起。查看其他人编写的程序可以为你提供构建条理清晰的代码的新思路。这就是为什么作为一个老师，我会花很多时间和学生们在一起，向他们展示我搜罗来的清晰算法模块的原因。
程序员的工作节奏 我的步调在程序开发的各个阶段是不同的。在某些时候，代码如泉涌一般，所有的东西都同时出现在脑海中：所有的变量名，变量之间的相互联系，指针从哪里开始、在哪里结束，磁盘的访问等。各种各样的事情都浮现在脑海里，因为我不停地修改自己的想法，所以没有办法写在纸上。我花在设计上的时间比花在编码上的时间多，而且我从来没有在合理的时间内完成过一个项目。
当数据结构还在雏形时，需要高度集中注意力，让它们在脑海中成形。在这个阶段我通常会在早上3点开始工作，一直干到大约下午6点，然后吃晚饭，早一点上床睡觉，再很早地起床，不断推敲构思，直到数据结构定下来。
在平静的时候，我的工作节奏会放松一些，我会提出下一阶段的解决方案。我会有步骤地去解决问题，先把问题排好次序，然后一次一个步骤地去解决——步骤A，步骤B，然后步骤C。我试过了，除非把步骤B做完，否则就无法做步骤C。
比尔.盖茨 人物介绍 作为微软的CEO，威廉·H.</description>
    </item>
    
    <item>
      <title>Java 简易RPC框架</title>
      <link>/2018/java-rpc/</link>
      <pubDate>Tue, 06 Nov 2018 23:08:41 +0800</pubDate>
      
      <guid>/2018/java-rpc/</guid>
      <description>需求分析 RPC 全称 Remote Procedure Call ，简单地来说，它能让使用者像调用本地方法一样，调用远程的接口，而不需要关注底层的具体细节。 例如车辆违章代办功能，如果车辆因为某种原因违章，只需要通过这个违章代办功能（它也许是个APP），我们就能动动手指，而省去了一些跑腿的工作。
不像微服务背景下大家所说的 RPC 框架，如 Dubbo 之类。这个 RPC 框架不提供过多的关于服务注册、服务发现、服务管理等功能。它针对的是这样的一些场景：在内部网络，或者局域网内，两个属于同个业务的系统之间需要通信，而我们又觉得去设计多一种二进制网络协议过于繁琐并且没有必要，这时候如果给客户端开发者一些明确的接口，让他知道实现什么功能该调用什么接口，那么省去的工作量以及开发效率上的提升不言而喻。
这个 RPC 系统基于 Java 语言实现，需求如下：
 RPC 服务端可以通过一条长连接发布多个接口（Interface），客户端按需生成对应接口的代理。 RPC 客户端也可以发布接口，以便在必要的时候，服务端可以主动调用客户端的接口实现 客户端与服务端之间保持长连接并且维持心跳 服务端针对不同的接口实现，可以指定不同的线程池去处理 序列化协议支持扩展 通信协议与具体编程语言无关 支持并发调用，一个RPC客户端实例要求是线程安全的  通信协议设计 高效的通信协议一般是二进制格式的，比较常见的还有文本协议比如说HTTP，为了追求效率，这个 RPC 框架就采用二进制格式。
协议的基本要素 魔数 要了解到，报文是在网络上传输的，安全性比较低，因此有必要采取一些措施使得并不是任何人都可以随随便便往我们的端口上发东西，因此我们对报文要有一个初步的识别功能，这时候“魔数(magic number)”就派上用场了。魔数并不受任何规范约束，没有人可以要求你的魔数应该遵循什么规范，实际上魔数只是我们通信双方都约定的一个“暗号”，不知道这个暗号的人就无法参与进通信中。例如 Java 源文件编译后的 class 文件开头就有一个魔数：0xCAFEBABE，随随便便打开一个class文件用十六进制编辑器查看，就能看到。
Java 虚拟机加载 class 的时候会先验证魔数。如果不是 CAFEBABE 就认为是不合法的 class 文件，并拒绝加载。 不过魔数起到的安全防范作用是非常有限的，“有心人”可以通过抓取网络包就识别出魔数了。因此魔数这个东西其实是“防君子不防小人”。
协议版本 一个协议可能也会有多个版本，例如说 HTTP1.0 和 HTTP1.1，不同版本的协议元素可能发生了改变，解析方式也会发生改变，因此协议设计这一块，需要预留出地方声明协议的版本，通信双方在解析协议或者拼装协议的时候才有迹可循。
报文类型 对于RPC框架来说，报文可能有多种类型：心跳类型报文、认证类型报文、请求类型报文、响应类型报文等。
上下文 ID RPC 调用其实是一个“请求-响应”的过程，并且跨物理机器，因此每次请求和响应，都必须带上上下文 ID，通信双方才能把请求和响应对应起来。
状态 状态用来标识一次调用时正常结束还是异常结束，通常由被调用方置状态。
请求数据 即发送到服务端的调用请求，通常是序列化后的二进制流，长度不定。
长度编码字段 收报文的一方怎么知道发报文的那一方发了多少字节呢？因此发送方必须在协议里告诉接收方需要接受多少字节才算一个完整的报文。</description>
    </item>
    
    <item>
      <title>Java 中的 Monitor 机制</title>
      <link>/2018/java-monitor/</link>
      <pubDate>Wed, 12 Sep 2018 18:24:59 +0800</pubDate>
      
      <guid>/2018/java-monitor/</guid>
      <description>monitor的概念 管程，英文是 Monitor，也常被翻译为“监视器”，monitor 不管是翻译为“管程”还是“监视器”，都是比较晦涩的，通过翻译后的中文，并无法对 monitor 达到一个直观的描述。 在《操作系统同步原语》 这篇文章中，介绍了操作系统在面对 进程/线程 间同步的时候，所支持的一些同步原语，其中 semaphore 信号量 和 mutex 互斥量是最重要的同步原语。 在使用基本的 mutex 进行并发控制时，需要程序员非常小心地控制 mutex 的 down 和 up 操作，否则很容易引起死锁等问题。为了更容易地编写出正确的并发程序，所以在 mutex 和 semaphore 的基础上，提出了更高层次的同步原语 monitor，不过需要注意的是，操作系统本身并不支持 monitor 机制，实际上，monitor 是属于编程语言的范畴，当你想要使用 monitor 时，先了解一下语言本身是否支持 monitor 原语，例如 C 语言它就不支持 monitor，Java 语言支持 monitor。 一般的 monitor 实现模式是编程语言在语法上提供语法糖，而如何实现 monitor 机制，则属于编译器的工作，Java 就是这么干的。
monitor 的重要特点是，同一个时刻，只有一个 进程/线程 能进入 monitor 中定义的临界区，这使得 monitor 能够达到互斥的效果。但仅仅有互斥的作用是不够的，无法进入 monitor 临界区的 进程/线程，它们应该被阻塞，并且在必要的时候会被唤醒。显然，monitor 作为一个同步工具，也应该提供这样的管理 进程/线程 状态的机制。想想我们为什么觉得 semaphore 和 mutex 在编程上容易出错，因为我们需要去亲自操作变量以及对 进程/线程 进行阻塞和唤醒。monitor 这个机制之所以被称为“更高级的原语”，那么它就不可避免地需要对外屏蔽掉这些机制，并且在内部实现这些机制，使得使用 monitor 的人看到的是一个简洁易用的接口。</description>
    </item>
    
    <item>
      <title>操作系统同步原语</title>
      <link>/2018/sync-primitive/</link>
      <pubDate>Wed, 12 Sep 2018 17:33:59 +0800</pubDate>
      
      <guid>/2018/sync-primitive/</guid>
      <description>竞态条件 在一般的操作系统中，不同的进程可能会分享一块公共的存储区域，例如内存或者是硬盘上的文件，这些进程都允许在这些区域上进行读写。 操作系统有一些职责，来协调这些使用公共区域的进程之间以正确的方式进行想要的操作。这些进程之间需要通信，需要互相沟通，有商有量，才能保证一个进程的动作不会影响到另外一个进程正常的动作，进而导致进程运行后得不到期望的结果。在操作系统概念中，通常用 IPC（Inter Process Communication，即进程间通信）这个名词来代表多个进程之间的通信。 为了解释什么是竞态条件（race condition），我们引入一个简单的例子来说明： 一个文件中保存了一个数字 n，进程 A 和进程 B 都想要去读取这个文件的数字，并把这个数字加 1 后，保存回文件。假设 n 的初始值是 0，在我们理想的情况下，进程 A 和进程 B 运行后，文件中 n 的值应该为 2，但实际上可能会发生 n 的值为 1。我们可以考量一下，每个进程做这件事时，需要经过什么步骤：
 读取文件里 n 的值 令 n = n + 1 把新的 n 值保存回文件。  在进一步解释竞态条件之前，必须先回顾操作系统概念中的几个知识点：
 进程是可以并发运行的，即使只有一个 CPU 的时候） 操作系统的时钟中断会引起进程运行的重新调度， 除了时钟中断，来自其它设备的中断也会引起进程运行的重新调度  假设进程 A 在运行完步骤 1 和 2，但还没开始运行步骤 3 时，发生了一个时钟中断，这个时候操作系统通过调度，让进程 B 开始运行，进程 B 运行步骤 1 时，发现 n 的值为 0，于是它运行步骤 2 和 3，最终会把 n = 1 保存到文件中。之后进程 A 继续运行时，由于它并不知道在它运行步骤 3 之前，进程 B 已经修改了文件里的值，所以进程 A 也会把 n = 1 写回到文件中。这就是问题所在，进程 A 在运行的过程中，会有别的进程去操作它所操作的数据。</description>
    </item>
    
    <item>
      <title>Java 线程的实现方式</title>
      <link>/2018/java-thread-impl/</link>
      <pubDate>Thu, 06 Sep 2018 18:28:28 +0800</pubDate>
      
      <guid>/2018/java-thread-impl/</guid>
      <description>进程与线程 在传统的操作系统中，最核心的概念是“进程”，进程是对正在运行的程序的一个抽象。 进程的存在让“并行”成为了可能，在一个操作系统中，允许运行着多个进程，这些进程“看起来”是同时在运行的。 如果我们的计算机同时运行着 web 浏览器、电子邮件客户端、即时通讯软件例如QQ微信等多个进程，我们感觉这些进程都是同时在运行的，假设这台计算机搭配的是多个 CPU 或者 多核 CPU，那么这种多个进程并行的现象可能一点也不奇怪，完全可以为每个进程单独分配一个 CPU，这样就实现了多进程并行。 然而事实上，在计算机只有一个 CPU 的情况下，它也能给人类一种感觉：多个进程同时在运行。但人类的感觉往往是比较模糊的，不精确的。事实是由于 CPU 的计算速度非常地快，它能快速地在各个进程之间切换，在某一瞬间，CPU 只能运行一个进程，但一秒钟之内，它就能通过快速切换，让人产生多个进程同时在运行的错觉。 在操作系统中，为什么在进程的基础上，又衍生出了线程的概念呢？
 由于对于一些进程而言，它内部会发生多种活动，有些活动可能会在某个时间里阻塞，有些活动不会，如果通过线程将这些活动分离开使它们能够并行地运行，则设计程序的时候会更加简单。 线程比进程的创建更加轻量级，性能消耗更少 如果一个进程既需要 CPU 计算，也需要I/O处理，拥有多线程允许这些活动重叠进行，加快整个进程的执行速度。  每一个进程在操作系统中都拥有独立的一块内存地址空间，该进程创建的所有线程共享这块内存，支持多线程的操作系统，会让线程作为 CPU 调度的最小单位。CPU 的时间片在不同的线程之间进行分配。
线程的可能实现方式 基本上主流的操作系统都支持线程，也提供了线程的实现。而 Java 语言为了应对不同硬件和操作系统的差异，提供了对线程操作的统一抽象，在 Java 中我们使用 Thread 类来代表一个线程。 Thread 的具体实现可能会有不同的实现方式：
使用内核线程实现 内核线程是操作系统内核支持的线程，在内核中有一个线程表用来记录系统中的所有线程，创建或者销毁一个线程时，都需要涉及到系统调用，然后再内核中对线程表进行更新操作。对内核线程的阻塞以及其它操作，都涉及到系统调用，系统调用的代价都比较大，涉及到在用户态和内核态之间的来回切换。此外，内核内部有线程调度器，用于决定应该将 CPU 时间片分配个哪个线程。 程序一般不会直接操作内核线程，而是使用内核线程的一种高级接口，轻量级进程。轻量级进程与内核线程之间的关系是 1：1，每一个轻量级进程内部都有一个内核线程支持。
上图中， LWP 指 Light Weight Process，即轻量级进程；KLT 指 Kernel Level Thread，即内核线程。
使用用户线程实现 用户线程是程序或者编程语言自己实现的线程库，系统内核无法感知到这些线程的存在。用户线程的建立、同步、销毁和调度，都在用户态中完成，无须内核的帮助，不需要进行系统调用，这样的好处是对于线程的操作是非常高效的。在这种情况下，进程和用户线程的比例是 1 ：N。
用户态线程面对如何阻塞线程时，会面临困难，阻塞一个用户态线程会出现把整个进程都阻塞的情况，多线程也就失去了意义。因为缺少内核的支持，所以很多需要利用内核才能完成的工作，例如阻塞与唤醒线程、多 CPU 环境下线程的映射等，都需要用户程序去实现，实现起来会异常困难。
使用用户线程和内核线程混合实现 在这种混合实现下，既存在用户线程，也存在内核线程。用户态线程的创建、切换这些操作依然很高效，并且用户态实现的线程，比较容易加大线程的规模。需要操作系统内核支持的功能，则通过内核线程来做到，例如映射到不同的处理器上、处理线程的阻塞与唤醒以及内核线程的调度等。这种实现依然会使用到轻量级进程 LWP，它是用户线程和内核线程之间的桥梁。
Java 线程的实现 在 JDK1.2 之前， Java 的线程是使用用户线程实现的，在 JDK1.</description>
    </item>
    
    <item>
      <title>线程安全实现与 CLH 队列</title>
      <link>/2018/clh-queue/</link>
      <pubDate>Wed, 05 Sep 2018 17:40:29 +0800</pubDate>
      
      <guid>/2018/clh-queue/</guid>
      <description>阻塞同步 在 Java 中，我们经常使用 synchronized 关键字来做到互斥同步以解决多线程并发访问共享数据的问题。synchronzied 关键字在编译后，会在 synchronized 所包含的同步代码块前后分别加入 monitorenter 和 monitorexit 这两个字节码指令。synchronized 关键字需要指定一个对象来进行加锁和解锁。例如：
public class Main { private static final Object LOCK = new Object(); public static void fun1() { synchronized (LOCK) { // do something  } } public static void fun2() { synchronized (LOCK) { // do something  } } } 在没有明确指定该对象时，根据 synchonized 修饰的是实例方法还是静态方法，从而决定是采用对象实例或者类的class实例作为所对象。例如：
public class SynchronizedTest { public synchronized void doSomething() { //采用实例对象作为锁对象  } } public class SynchronizedTest { public static synchronized void doSomething() { //采用SynchronizedTest.</description>
    </item>
    
    <item>
      <title>分布式事务概览</title>
      <link>/2018/distributed-transactions-intro/</link>
      <pubDate>Mon, 27 Aug 2018 22:50:28 +0800</pubDate>
      
      <guid>/2018/distributed-transactions-intro/</guid>
      <description>传统的事务 事务(Transaction)是访问并可能更新数据库中各种数据项的一个程序执行单元(unit)。在关系数据库中，一个事务由一组SQL语句组成。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。
 原子性：一个事务是一个不可分割的工作单位，事务中包括的操作要么全部完成，要么全部取消 一致性：事务使数据库从一个一致性状态转换为另一个一致性状态，事务的中间状态不可被观察到。 隔离性：一个事务内部的操作及使用的数据对并发中的其它事务是隔离的。 持久性：一个事务一旦被提交，其对数据库中数据的改变是永久性的，不因其它故障而受影响。  集中式数据库与分布式数据库 ACID 以一个学生管理系统为例，可以看到对于这个学生成绩管理系统，不同的系统使用角色，对它有不同的要求：
那么传统的事务 ACID属性，是如何帮助我们做到满足这些需求的呢？
数据量变大 而实际上，当我们考虑更大的数据量的时候，会发现，依赖于传统集中式数据库并无法很好地满足我们对系统的需求。当数据量变大，第一个要面对的问题是数据再也无法只放在一台机器上了，数据量的变大，意味着会有越来越多的客户端来查询这个数据库，那么就会出现查询的瓶颈，并且如果数据数非常重要的话，那么这些数据时丢不起的，而如果仅依赖于集中式数据库，那么当这个集中式数据库发生一些致命性的状况的时候，数据可能丢失。总而言之，对于集中式数据库来说，如果它出现了什么问题：不可查、丢数据等。那么其实意味着整个业务系统都陷入了不可用的状态。
总结下来，集中式数据库在通信、系统可靠性、可扩展性、性能瓶颈以及设计管理上，存在着明显的缺点：
 集中数据库会有多个成绩录入员，因为集中数据库只存在于某个服务器上，而成绩录入员却是分散在全国各地的，因此会造成额外的通信开销。 由于集中式数据库所有的数据都存在一个点上，那么一旦这个点发生故障，就会导致整个成绩管理系统停止运作，系统的可靠性差。 随着数据量的变大，录入的客户端变多，查询的客户端变多，那么存储系统本身的性能可能就会成为瓶颈，包括 CPU计算能力，IO吞吐能力，存储能力，都有可能成为瓶颈。 可扩展性差，正由于集中式数据库存在着性能差的问题，因此只能通过升级单机硬件能力的方式，实现数据库服务能力扩展，比如原来采用 MySQL 单机数据库，遇到访问瓶颈时更换磁盘，访问量更高时就需要考虑使用 Oracle 的商用解决方案、高端的存储设备、高端小型机，也就是 IOE 架构，甚至升级 IOE 设备，以换取更高的扩展和服务能力，这个过程就会存在设备升级和数据迁移的成本，其可扩展性的代价会面临巨大的成本问题。此外，根据摩尔定律，单机硬件能力的升级，并不能换来等比的效率加速比例，也就是说，增加多一倍的CPU核数，并不能带来一倍的性能提升，这个提升并不是线性的。 当一个系统的功能变得越来越复杂，例如说b不仅记录学生的成绩，还记录学生的奖惩历史，出勤情况，而数据库仍然只有一个点的情况下，集中数据库上承载的业务类型越来越多，导致管理困难。  传统集中式数据库虽然能够很好地保证业务一致性，但其面临高速增长的访问量和数据量时存在性能和处理能力上的瓶颈。
数据分布式存储 分布式数据库虽然引进了复杂性例如分布式事务的问题，但是分布式数据库能解决集中式数据库的大多数痛点。 分布式数据库与集中式数据库的区别主要在数据分布和可扩展性两方面：
 分布式数据库的数据分散存储，集中式数据库的数据集中存储。 分布式数据库的扩展高效并且性价比高，而集中式数据库不能无限扩容并且扩容存在着成本导致的性价比的问题。  总结起来，分布式数据库具有以下的特点：
 数据分布性，数据可以分布在不同的机器上，不同地理位置上。 数据统一性，虽然数据存放在不同的机器上，不同的地理位置上，但从整体上来看，它的系统逻辑应该是一致的 数据的透明性，虽然数据分散了，但是无论是查询还是更新，它们都应该有统一的入口 数据的安全性，单个数据节点如果出现错误，它不应该影响其它节点，从而数据库整体的安全性 数据的可扩展性，当现有集群称为瓶颈时，分布式数据库系统可以通过扩容来解决可扩展性的问题 数据的自治性，虽然数据分散存储，但每一个节点它都应该要能够独立管理自己的数据，同时又不影响整体的统一性。  分布式事务 在高速增长的访问量和数据量的背景下，为了解决单机性能瓶颈以及可扩展性等问题，数据库分库分表拆分和服务化（微服务）的运用越来越广泛。完成一个业务功能，可能需要横跨多个服务或者横跨多个数据库节点；也就是说，需要操作的资源位于多个资源服务器上，从业务的角度来看，需要保证对多个资源服务器的操作，要么全部成功，要么全部失败。从本质上来说，分布式事务要保证不同资源服务器上的数据一致性。
场景 典型的分布式事务场景主要有跨库事务、分库分表以及跨服务事务。
跨库事务 分库分表 当对数据库通过中间件代理的形式进行水平拆分后，不可避免的会在一个事务中操作多个分片节点 跨服务 在服务化的架构下，完成业务功能可能涉及到对多个服务的调用，而这些服务分别会操作不同的数据库。需要保证跨服务对数据库的操作要么都成功，要么都失败，这是服务化场景下面临的分布式问题。 X/Open DTP模型与XA规范 DTP 模型 X/Open DTP(X/Open Distributed Transaction Processing Reference Model) 是X/Open 这个组织定义的一套分布式事务的标准，也就是了定义了规范和API接口，由厂商进行具体的实现。</description>
    </item>
    
    <item>
      <title>Netty4.x ByteBuf 基本机制及其骨架实现</title>
      <link>/2017/netty-bytebuf/</link>
      <pubDate>Fri, 07 Jul 2017 22:35:04 +0800</pubDate>
      
      <guid>/2017/netty-bytebuf/</guid>
      <description>概述 netty 是一个 NIO 框架，在 JDK API 已提供相对直接的 NIO Library 的情况下，几乎很少的软件系统会直接用 NIO 进行编程，也很少有开发者会直接使用 NIO 技术开发网络相关的程序。因为 native nio library 已饱受诟病，API 难用，容易出错，存在一些声称解决但还没解决的 bug（bug id = 6403933，JDK 1.7 声称解决了该 Bug，但实际上只是降低了该 bug 发生的概率），使用 native nio library 来开发可靠性、鲁棒性高的网络程序，工作量以及出错率都要更高，使用 netty 框架，就是为了解决这些问题。
ByteBuffer 的忏悔 基于 NIO 非阻塞模型的编程，基本上是面向数据容器编程，BIO 与 NIO 除了它们在阻塞 IO 线程方面有所不同外，它们在操作数据方面是有一些共性的，那就是从网络流中读数据，并放入一个容器中。 对于 BIO 来说，大多数时候，这个容器就是一个字节数组
byte[] buf = new byte[8196]; int cnt = 0; while ((cnt = input.read(buf)) != -1) { //...... } 在这里，容器就是指 buf 这个字节数组。而在 NIO 中，容器是指 ByteBuffer，由于 NIO 编程的复杂性，需要解决类似于 TCP 半包问题等，因此对这个容器的要求不仅仅是“存储数据”那么简单，还希望这个容器能提供另外的功能，这是 ByteBuffer 存在的原因，它提供了一些方便的 API，让开发者操作底层的字节数组。 然而 ByteBuffer 存在几个不得人心的缺点：</description>
    </item>
    
    <item>
      <title>redis 主从配置</title>
      <link>/2017/redis-master-slave-config/</link>
      <pubDate>Wed, 07 Jun 2017 22:46:50 +0800</pubDate>
      
      <guid>/2017/redis-master-slave-config/</guid>
      <description>概述 Redis的replication机制允许slave从master那里通过网络传输拷贝到完整的数据备份。具有以下特点：
 异步复制。从2.8版本开始，slave能不时地从master那里获取到数据。 允许单个master配置多个slave slave允许其它slave连接到自己。一个slave除了可以连接master外，它还可以连接其它的slave。形成一个图状的架构。 master在进行replication时是非阻塞的，这意味着在replication期间，master依然能够处理客户端的请求。 slave在replication期间也是非阻塞的，也可以接受来自客户端的请求，但是它用的是之前的旧数据。可以通过配置来决定slave是否在进行replication时用旧数据响应客户端的请求，如果配置为否，那么slave将会返回一个错误消息给客户端。不过当新的数据接收完全后，必须将新数据与旧数据替换，即删除旧数据，在替换数据的这个时间窗口内，slave将会拒绝客户端的请求和连接。 一般使用replication来可以实现扩展性，例如说，可以将多个slave配置为“只读”，或者是纯粹的数据冗余备份。 能够通过replication来避免master每次持久化时都将整个数据集持久化到硬盘中。只需把master配置为不进行save操作(把配置文件中save相关的配置项注释掉即可)，然后连接上一个slave，这个slave则被配置为不时地进行save操作的。不过需要注意的是，在这个用例中，必须确保master不会自动启动。更多详情请继续往下读。  Master持久化功能关闭时Replication的安全性 当有需要使用到replication机制时，一般都会强烈建议把master的持久化开关打开。即使为了避免持久化带来的延迟影响，不把持久化开关打开，那么也应该把master配置为不会自动启动的。
为了更好地理解当一个不进行持久化的master如果允许自动启动所带来的危险性。可以看看下面这种失败情形：
 假设我们有一个redis节点A，设置为master，并且关闭持久化功能，另外两个节点B和C是它的slave，并从A复制数据。 如果A节点崩溃了导致所有的数据都丢失了，它会有重启系统来重启进程。但是由于持久化功能被关闭了，所以即使它重启了，它的数据集是空的。 而B和C依然会通过replication机制从A复制数据，所以B和C会从A那里复制到一份空的数据集，并用这份空的数据集将自己本身的非空的数据集替换掉。于是就相当于丢失了所有的数据。
 即使使用一些HA工具，比如说sentinel来监控master-slaves集群，也会发生上述的情形，因为master可能崩溃后迅速恢复。速度太快而导致sentinel无法察觉到一个failure的发生。
当数据的安全很重要、持久化开关被关闭并且有replication发生的时候，那么应该禁止实例的自启动。
replication工作原理 如果你为master配置了一个slave，不管这个slave是否是第一次连接上Master，它都会发送一个SYNC命令给master请求复制数据。
master收到SYNC命令后，会在后台进行数据持久化，持久化期间，master会继续接收客户端的请求，它会把这些可能修改数据集的请求缓存在内存中。当持久化进行完毕以后，master会把这份数据集发送给slave，slave会把接收到的数据进行持久化，然后再加载到内存中。然后，master再将之前缓存在内存中的命令发送给slave。
当master与slave之间的连接由于某些原因而断开时，slave能够自动重连Master，如果master收到了多个slave并发连接请求，它只会进行一次持久化，而不是一个连接一次，然后再把这一份持久化的数据发送给多个并发连接的slave。
当master和slave断开重连后，一般都会对整份数据进行复制。但从redis2.8版本开始，支持部分复制。
数据部分复制 从2.8版本开始，slave与master能够在网络连接断开重连后只进行部分数据复制。
master会在其内存中创建一个复制流的等待队列，master和它所有的slave都维护了复制的数据下标和master的进程id，因此，当网络连接断开后，slave会请求master继续进行未完成的复制，从所记录的数据下标开始。如果进程id变化了，或者数据下标不可用，那么将会进行一次全部数据的复制。
支持部分数据复制的命令是PSYNC
不需硬盘参与的Replication 一般情况下，一次复制需要将内存的数据写到硬盘中，再将数据从硬盘读进内存，再发送给slave。
对于速度比较慢的硬盘，这个操作会给master带来性能上的损失。Redis2.8版本开始，实验性地加上了无硬盘复制的功能。这个功能能将数据从内存中直接发送到slave，而不用经过硬盘的存储。
 不过这个功能目前处于实验阶段，还未正式发布。
 相关配置 与replication相关的配置比较简单，只需要把下面一行加到slave的配置文件中：
slaveof 192.168.1.1 6379 你只需要把ip地址和端口号改一下。当然，你也可以通过客户端发送SLAVEOF命令给slave。
部分数据复制有一些可调的配置参数，请参考redis.conf文件。
无硬盘复制功能可以通过repl-diskless-sync来配置，另外一个配置项repl-diskless-sync-delay用来配置当收到第一个请求时，等待多个slave一起来请求之间的间隔时间。
只读的slave 从redis2.6版本开始，slave支持只读模式，而且是默认的。可以通过配置项slave-read-only来进行配置，并且支持客户端使用CONFIG SET命令来动态修改配置。
只读的slave会拒绝所有的写请求，只读的slave并不是为了防范不可信的客户端，毕竟一些管理命令例如DEBUG和CONFIG在只读模式下还是可以使用的。如果确实要确保安全性，那么可以在配置文件中将一些命令重新命名。
也许你会感到很奇怪，为什么能够将一个只读模式的slave恢复为可写的呢，尽管可写，但是只要slave一同步master的数据，就会丢失那些写在slave的数据。不过还是有一些合法的应用场景需要存储瞬时数据会用到这个特性。不过，之后可能会考虑废除掉这个特性。
Setting a slave to authenticate to a master
如果master通过requirepass配置项设置了密码，slave每次同步操作都需要验证密码，可以通过在slave的配置文件中添加以下配置项：
masterauth &amp;lt;password&amp;gt; 也可以通过客户端在运行时发送以下命令：
config set masterauth &amp;lt;password&amp;gt; 至少N个slave才允许向master写数据 从redis2.8版本开始，master可以被配置为，只有当master当前有至少N个slave连接着的时候才接受写数据的请求。
然而，由于redis是异步复制的，所以它并不能保证slave会受到一个写请求，所以总有一个数据丢失的时间窗口存在。
这个机制的工作原理如下所示：
 slave每秒发送ping心跳给master，询问当前复制了多少数据。 master会记录下它上次收到某个slave的ping心跳是什么时候。 使用者可以配置一个时间，来指定ping心跳的发送不应超过的一个超时时间  如果master有至少N个slave，并且ping心跳的超时不超过M秒，那么它就会接收写请求。</description>
    </item>
    
    <item>
      <title>redis sort 命令详解</title>
      <link>/2017/redis-sort-explain/</link>
      <pubDate>Wed, 07 Jun 2017 22:24:47 +0800</pubDate>
      
      <guid>/2017/redis-sort-explain/</guid>
      <description>基本使用 命令格式：
SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern ...]] [ASC|DESC] [ALPHA] [STORE destination] 默认情况下，排序是基于数字的，各个元素将会被转化成双精度浮点数来进行大小比较，这是SORT命令最简单的形式，也就是下面这种形式：
SORT mylist 如果mylist是一个包含了数字元素的列表，那么上面的命令将会返回升序排列的一个列表。如果想要降序排序，要使用DESC描述符，如下所示：
SORT mylist DESC 如果mylist包含的元素是string类型的，想要按字典顺序排列这个列表，那么就要用到ALPHA描述符，如下所示：
SORT mylist ALPHA Redis是基于UTF-8编码来处理数据的, 要确保你先设置好了!LC_COLLATE环境变量。
对于返回的元素个数也是可以进行限制的，只需要使用LIMIT描述符。使用这个描述符，你需要提供偏移量参数，来指定需要跳过多少个元素，返回多少个元素。 下面这个例子将会返回一个已经排序好了的列表中的10个元素，从下标为0开始：
SORT mylist LIMIT 0 10 几乎全部的描述符都可以同时使用。例如下面这个例子所示，它将会返回前5个元素，以字典顺序降序排列：
SORT mylist LIMIT 0 5 ALPHA DESC 通过外部key来排序 有时候，你会想要用外部的key来作为权重去排列列表或集合中的元素，而不是使用列表或集合中本来就有的元素来排列。 下面以一个例子来解释： 假设现在有一张这样的表，有商品id，商品价格，以及商品的重量。
   gid price_{gid} weight_{gid}     1 20 3   2 40 2   3 30 4   4 10 1    首先将上述表格的数据导入到redis中(redis版本：2.</description>
    </item>
    
    <item>
      <title>Druid SQL 解析器</title>
      <link>/2017/druid-sql-parser/</link>
      <pubDate>Wed, 07 Jun 2017 17:43:33 +0800</pubDate>
      
      <guid>/2017/druid-sql-parser/</guid>
      <description>认识 Druid Druid 是阿里巴巴公司开源的一个数据库连接池，它的口号是：为监控而生的数据库连接池
根据官方 wiki的介绍
 Druid 是一个 JDBC 组件库，包括数据库连接池、SQL Parser 等组件，DruidDataSource 是最好的数据库连接池。
 显然，官方有意无意地强调了 DruidDataSource 是最好的数据库连接池 -_- &amp;hellip;
Druid SQL 解析器 Druid 作为一个数据库连接池，功能很多，但我接触 Druid 的时候，却不是因为它有世界上最好的数据库连接池实现。而是因为有些开源项目(比如，mycat)，借用了 Druid 的 SQL 解析功能。我需要研究这个开源项目，发现作为一个数据库中间件，它的 SQL 解析功能是直接引用的 Druid，Druid 包除了 SQL 解析模块的代码外，其它的代码并没有使用到。而这部分代码显然让人在研究 SQL 解析器代码时容易分心，产生厚重感和焦虑感。
Druid 本来的代码结构如下： 提取 Druid SQL 解析器 在确认我并不需要使用到全世界最好的数据库连接池后，我想把除了 SQL 解析部分的代码全部剔除，仅仅留下 SQL 解析器模块。
一开始的做法当然是“暴力删除”，通过对代码的整体浏览，大概判断出哪些 package 与 SQL 解析有关，其余的直接删除。这样做会有些问题，比如说直接删除后在 IDE 中会立马浮现一些小红叉叉，但令人感到愉悦的是，Druid 的模块分解做得十分优秀，SQL 解析模块基本上作为一个工具模块，与其它模块实际上是分离的。
因此虽然是“暴力删除”，却也得到了一个令人满意的结果。
由于我只关注的是 Druid 对 MySQL 方言的解析，并且也不想看到 Druid 解析其它数据库方言的内容，也不愿被 Druid 那些为了适应多种数据库的“兼容性代码”混淆视听，因此狠下心来，把对其它 SQL 方言的支持也全都剔除，只留下与 MySQL 相关的代码。</description>
    </item>
    
    <item>
      <title>Java Trouble Shooting</title>
      <link>/2017/java-trouble-shooting/</link>
      <pubDate>Tue, 06 Jun 2017 18:33:55 +0800</pubDate>
      
      <guid>/2017/java-trouble-shooting/</guid>
      <description>什么是线程栈(thread dump) 线程栈是某个时间点，JVM所有线程的活动状态的一个汇总；通过线程栈，可以查看某个时间点，各个线程正在做什么，通常使用线程栈来定位软件运行时的各种问题，例如 CPU 使用率特别高，或者是响应很慢，性能大幅度下滑。
线程栈包含了多个线程的活动信息，一个线程的活动信息通常看起来如下所示：
&amp;#34;main&amp;#34; prio=10 tid=0x00007faac0008800 nid=0x9f0 waiting on condition [0x00007faac6068000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at ThreadDump.main(ThreadDump.java:4) 这条线程的线程栈信息包含了以下这些信息：
 线程的名字：其中 main 就是线程的名字，需要注意的是，当使用 Thread 类来创建一条线程，并且没有指定线程的名字时，这条线程的命名规则为 Thread-i，i 代表数字。如果使用 ThreadFactory 来创建线程，则线程的命名规则为 pool-i-thread-j，i 和 j 分别代表数字。 线程的优先级：prio=10 代表线程的优先级为 10 线程 id：tid=0x00007faac0008800 代表线程 id 为 0x00007faac0008800，而** nid=0x9f0** 代表该线程对应的操作系统级别的线程 id。所谓的 nid，换种说法就是 native id。在操作系统中，分为内核级线程和用户级线程，JVM 的线程是用户态线程，内核不知情，但每一条 JVM 的线程都会映射到操作系统一条具体的线程 线程的状态：java.lang.Thread.State: TIMED_WAITING (sleeping) 以及 waiting on condition 代表线程当前的状态 线程占用的内存地址：[0x00007faac6068000] 代表当前线程占用的内存地址 线程的调用栈：at java.lang.Thread.sleep(Native Method) 以及它之后的相类似的信息，代表线程的调用栈  回顾线程状态  NEW：线程初创建，未运行 RUNNABLE：线程正在运行，但不一定消耗 CPU BLOCKED：线程正在等待另外一个线程释放锁 WAITING：线程执行了 wait, join, park 方法 TIMED_WAITING：线程调用了sleep, wait, join, park 方法，与 WAITING 状态不同的是，这些方法带有表示时间的参数。  例如以下代码：</description>
    </item>
    
    <item>
      <title>Logback 使用详解</title>
      <link>/2017/logback-usage/</link>
      <pubDate>Fri, 10 Feb 2017 19:35:24 +0800</pubDate>
      
      <guid>/2017/logback-usage/</guid>
      <description>概览 简单地说，Logback 是一个 Java 领域的日志框架。它被认为是 Log4J 的继承人。 Logback 主要由三个模块组成：
 logback-core logback-classic logback-access  logback-core 是其它模块的基础设施，其它模块基于它构建，显然，logback-core 提供了一些关键的通用机制。logback-classic 的地位和作用等同于 Log4J，它也被认为是 Log4J 的一个改进版，并且它实现了简单日志门面 SLF4J；而 logback-access 主要作为一个与 Servlet 容器交互的模块，比如说 tomcat 或者 jetty，提供一些与 HTTP 访问相关的功能。
目前 Logback 的使用很广泛，很多知名的开源软件都使用了 Logback作为日志框架，比如说 Akka，Apache Camel 等。
Logback 与 Log4J 实际上，这两个日志框架都出自同一个开发者之手，Logback 相对于 Log4J 有更多的优点
 同样的代码路径，Logback 执行更快 更充分的测试 原生实现了 SLF4J API（Log4J 还需要有一个中间转换层） 内容更丰富的文档 支持 XML 或者 Groovy 方式配置 配置文件自动热加载 从 IO 错误中优雅恢复 自动删除日志归档 自动压缩日志成为归档文件 支持 Prudent 模式，使多个 JVM 进程能记录同一个日志文件 支持配置文件中加入条件判断来适应不同的环境 更强大的过滤器 支持 SiftingAppender（可筛选 Appender） 异常栈信息带有包信息  快速上手 想在 Java 程序中使用 Logback，需要依赖三个 jar 包，分别是 slf4j-api，logback-core，logback-classic。其中 slf4j-api 并不是 Logback 的一部分，是另外一个项目，但是强烈建议将 slf4j 与 Logback 结合使用。要引用这些 jar 包，在 maven 项目中引入以下3个 dependencies</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第十一章 连接池</title>
      <link>/2017/jdbc4.2-spec-11/</link>
      <pubDate>Mon, 02 Jan 2017 21:58:47 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-11/</guid>
      <description>在基本的 DataSource 实现中，客户端的 Connection 对象与物理数据库连接有着1:1的关系。当 Connection 被关闭以后，物理连接也会被关闭。因此，连接的频繁打开、初始化以及关闭，会在一个客户端会话中上演多次，带来了过重的性能消耗。 而连接池就能解决这个问题，连接池维护了一系列物理数据库连接的缓存，可以被多个客户端会话重复使用。连接池能够极大地提高性能和可扩展性，特别是在一个三层架构的环境中，大量的客户端可以共享一个数量比较小的物理数据库连接池。在图11-1中，JDBC 驱动提供了一个 ConnectionPoolDataSource 的实现，应用服务器可以用它来创建和管理连接池。
连接池的管理策略跟具体的实现有关，也跟具体的应用服务器有关。应用服务器对客户端提供了一个 DataSource 接口的具体实现，使得连接池化对于客户端来说是透明的。最终，客户端使用 DataSource API 就能和之前使用 JNDI 一样，获得了更好的性能和可扩展性。
下文将会介绍 ConnectionPoolDataSource 接口、PooledConnection 接口以及 ConnectionEvent 类，这三个组成部分是一个相互合作的关系，下文将以一个经典线程池的实现的角度，逐步描述这几部分。这一章也会介绍基本的 DataSource 对象和池化的 DataSource 对象之间的区别，此外，还会讨论一个池化的连接如何能够维护一堆可重用的 PreparedStatement 对象。
尽管本章中的所有讨论都是假设在三层架构环境下的，但连接的池化在两层架构的环境下也同样有用。 在两层架构的环境中，JDBC 驱动既实现了 DataSource 接口，也实现 ConnectionPoolDataSource 接口，这种实现方式允许客户端打开或者关闭多个连接。
11.1 ConnectionPoolDataSource 和 PooledConnection 一般来说， 一个 JDBC 驱动会去实现 ConnectionPoolDataSource 接口，应用服务器可以使用这个接口来获得 PooledConnection 对象，以下代码展示了 getPooledConnection 方法的两种版本
public interface ConnectionPoolDataSource { PooledConnection getPooledConnection() throws SQLException; PooledConnection getPooledConnection(String user, String password) throws SQLException; } 一个 PooledConnection 对象代表一条与数据源之间的物理连接。JDBC 驱动对于 PooledConnection 的实现，则会封装所有与维护这条连接相关的细节。 应用服务器则会在它的 DataSource 接口的实现中，缓存和重用这些 PooledConnection。当客户端调用 DataSource.</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第十章 事务</title>
      <link>/2017/jdbc4.2-spec-10/</link>
      <pubDate>Mon, 02 Jan 2017 21:55:52 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-10/</guid>
      <description>事务用来提供数据集成性、正确的应用语义，以及并发访问时数据的一致性视图。所有符合 JDBC 规范的驱动都必须支持事务，JDBC 的事务管理 API 参照 SQL:2003 标准并且包含了以下的概念：
 自动提交模式 事务隔离级别 Savepoints  本章讨论单个连接上的事务，涉及多条连接的事务将会在第十二章《分布式事务》中讨论。
10.1 事务边界和自动提交 什么时候应该开启一个事务，是 JDBC 驱动或者底层的数据源做的一个隐式的决定，尽管有一些数据源支持 begin transaction 语句，但这个语句没有对应的 JDBC API。当一条 SQL 语句要求开启一个事务并且当前没有事务未执行完，那么新事务就会被开启。 Connection 有一个属性 autocommit 来表明什么时候应该结束事务。如果 autocommit 启用，那么每一条 SQL 语句完全执行后，都会自动执行事务的提交。以下几种情况，视为完全执行：
 对于 DML 语句来说，例如 Insert，Update，Delete；以及 DDL 语句。这些语句在数据源端执行完毕就代表语句完全执行。 对于 Select 语句来说，完全执行意味着对应的结果集被关闭。 对于 CallableStatement 对象或者对于那些返回多个结果集的语句，完全执行意味着所有的结果集都关闭，以及所有的影响行数和出参都被获取到了。  10.1.1 关闭自动提交模式 以下代码示范了如何关闭自动提交模式：
// Assume con is a Connection object con.setAutoCommit(false); 当关闭自动提交，必须显式地调用 Connection 的 commit 方法提交事务或者调用 rollback 方法回滚事务。这种处理方式是合理的，因为事务的管理工作不是驱动应该做的，应用层应该自己管理事务，例如：
 当应用需要将一组 SQL 组成一个事务的时候 当应用服务器管理事务的时候  autocommit 的默认值为 true，如果在一个事务的过程中，autocommit 的值被改变了，那么将会导致当前事务被提交。如果调用了 setAutocommit 方法，但没有改变原来的值，则不会产生其它附加影响，相当于没有调过一样。</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第九章 连接</title>
      <link>/2017/jdbc4.2-spec-9/</link>
      <pubDate>Mon, 02 Jan 2017 21:52:29 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-9/</guid>
      <description>一个 Connection 对象，表示了与某个数据源的一条连接，数据源的种类可以是关系型数据库，文件系统等等之类，只要有对应的 JDBC 驱动，都可以称之为数据源。应用程序使用 JDBC API 来维护多条连接，这些连接可能访问的是多个数据源，也可能访问的只是一个数据源。从 JDBC 驱动的角度来看，一个 Connection 对象就意味着一个客户端会话，一个会话会保持许多状态，例如用户 ID，一系列的 SQL Statement 以及结果集，也保存了当前使用的事务处理策略。
可以通过以下两种方式之一来获取一条连接：
 使用 DriverManager 这个类以及各种各样的驱动实现 使用 DataSource 类  更推荐使用 DataSource 对象来获取连接，因为这增强了应用的可移植性，使得代码更容易维护了，并且使得对连接池和分布式事务的使用更加地透明。所有的 Java EE 组件，都会使用 DataSource 对象来获取连接。
这一章将会介绍各种不同的 JDBC 驱动以及如何使用 Driver 接口、DriverManager 类以及基本的 DataSource 接口。关于连接池和分布式事务的介绍分别在第11章和第12章做介绍。
9.1 驱动的种类  Type 1 这种类型的 JDBC 驱动是对另外一种访问 API 的映射，比如说 ODBC，一般需要依赖本地库，这就导致了它的可移植性不行。JDBC-ODBC 桥就是这种类型的驱动。 Type 2 这种类型的 JDBC 驱动一部分是用 Java 语言写的，一部分是用本地代码写的。这种驱动使用一个本地的客户端库来连接数据源。由于对本地代码的使用，可移植性也不行。 Type 3 这种类型的驱动使用纯 Java 语言编写，但是通信的时候需要经过一个中间件，使用的是与数据库具体协议无关的独立协议。这个中间件转发客户端的请求给后面的数据源。 Type 4 这种类型的驱动使用纯 Java 语言编写，并且使用网络协议或者文件 IO 与具体的数据源通信，客户端直接与数据源连接。  9.</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第八章 异常</title>
      <link>/2017/jdbc4.2-spec-8/</link>
      <pubDate>Mon, 02 Jan 2017 21:49:40 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-8/</guid>
      <description>当访问一个数据源时发生错误或者警告，JDBC 用 SQLException 这个类及其子类来表示并提供相关的异常信息。
8.1 SQLException SQLException 由一下几部分组成：
 描述错误的文本信息。可以通过 SQLException.getMessage() 来获取。 一个 SQLState 对象。可以通过 SQLException.getSQLStateType() 来获取。 错误码，是某种错误类型的一个编码，int 类型，可以通过 SQLException.getErrorCode() 来获取。 底层的异常，是一个 Throwable 对象，用来代表引起 SQLException 发生的真正原因，通过 SQLException.getCause() 来获取。 异常链的引用，如果有不止一个 SQLException 异常，可以通过递归式地调用 SQLException.getNextException 来获取整个异常链，直到这个方法返回 null，异常链结束。  8.1.1 对 Java SE 异常链的支持 SQLException 和它的子类都支持 Java SE 的异常链机制，为了实现这个功能，有以下几个地方特意做了处理：
 增加额外的四个构造函数 支持对异常链的 For-Each 语法 getCause 方法不一定会返回 SQLException 类型或者其子类型  可以参考 JDBC API Java DOC 获取更详细的信息
8.1.2 遍历多个 SQLException 在一次 SQL 语句的执行中，很有可能会发生一个或者多个异常，多个异常之间有着相应的联系，这意味着，当捕获到一个 SQLException 时，它的背后可能还有多个 SQLException 链接在它身上，为了遍历这条异常链，通常可以通过循环调用 SQLException.</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第七章 数据库元数据</title>
      <link>/2017/jdbc4.2-spec-7/</link>
      <pubDate>Mon, 02 Jan 2017 21:45:57 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-7/</guid>
      <description>JDBC 驱动需要实现 DatabaseMetaData 这个接口，以便向驱动的使用者提供一些关于底层数据源的信息。这个接口主要被应用服务器以及一些工具型代码使用，以决定如何与一个数据源交互，应用程序有时候也会使用这个接口里的方法去获得数据源的信息，但这种用法并不是典型的用法。
DatabaseMetaData 这个接口拥有超过 150 个方法，这么多的方法，可以根据它们提供的信息的类型进行分类，信息类型有：
 关于数据源通用的信息 关于数据源是否支持某个特性或者是否具有某种能力的信息 数据源的限制 数据源有哪些 SQL 对象，以及这些 SQL 对象都拥有哪些属性 数据源是否支持事务  DatabaseMetaData 接口也拥有超过 40 个字段，当调用接口的方法时，这些字段常量会作为返回值。
本章会对 DatabaseMetaData 做一个概览，给出一些实例来验证接口定义的方法，并介绍一些新的方法。如果希望更深入地理解所有的方法，建议读者参考 JDBC API Specification。
 注意，JDBC 也定义了 ResultSetMetaData 接口，这个接口我们将会在第十五章遇见它。
 7.1 创建一个 DatabaseMetaData 对象 通过 Connection 接口的 getMetaData 方法来创建一个 DatabaseMetaData  对象。一旦这个对象创建完成，就可以使用这个对象动态地查询与底层数据源有关的信息。以下代码示例创建了一个 DatabaseMetaData 对象，并使用这个对象来获取底层数据源支持的表名最大字符数是多少
// 在这里 con 是一个 Connection 对象 DatabaseMetaData dbmd = con.getMetadata(); int maxLen = dbmd.getMaxTableNameLength(); 7.2 获取通用的信息 有一些 DatabaseMetaData 接口的方法用来动态地获取底层数据源的一些通用的信息和实现细节。例如以下这些方法：
 getURL getUserName getDatabaseProductVersion, getDriverMajorVersion 和 getDriverMinorVersion getSchemaTerm, getCatalogTerm 和 getProcedureTerm nullsAreSortedHigh 和 nullsAreSortedLow usesLocalFiles 和 usesLocalFilePerTable getSQLKeyword  7.</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第六章 遵守规范</title>
      <link>/2017/jdbc4.2-spec-6/</link>
      <pubDate>Mon, 02 Jan 2017 21:41:10 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-6/</guid>
      <description>本章指出了实现一个 JDBC 驱动所需要遵守的规范，在本章中没有指出的规范，则作为可选项来遵守。
6.1 准则与要求 以下的准则是 JDBC API 规范要求实现者遵守的基本准则
  JDBC API 的实现者必须支持 Entry Level SQL92 标准，以及  Drop Table 命令。对 Entry Level SQL92 标准的支持是实现 JDBC API 的最小要求，对于 SQL99 和 SQL2003 特性的实现，必须遵照 SQL99 和 SQL2003 的规范。
  JDBC 驱动必须支持转义语法，转义语法在 第十三章 中有详细解释。
  JDBC 驱动必须支持事务，参考 第十章。
  如果 DatabaseMetaData 的某个方法指明某个特性的可用的，那么驱动必须根据这个特性的相关规范中规定的标准语法实现这个特性，如果该特性需要使用到数据源的原生 API 或者是 SQL 方言，那么由驱动负责实现从标准 SQL 语法到原生 API 或者 SQL 方言的映射关系。如果支持了某个特性，那么 DatabaseMetaData 中与这个特性相关的方法也必须提供实现。比如说，如果一个驱动实现了 RowSet 接口，那么它也应该实现 RowSetMetaData 接口。
  驱动必须提供对底层数据源特性的访问方式，包括扩展了 JDBC API 的特性。这么规定的目的是能让使用了 JDBC API 的应用程度能像数据源的原生程序一样，访问与数据源有关的特性。</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第五章 类与接口</title>
      <link>/2017/jdbc4.2-spec-5/</link>
      <pubDate>Mon, 02 Jan 2017 13:55:10 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-5/</guid>
      <description>以下的类和接口，组成了 JDBC API
5.1 java.sql包 JDBC API 的核心部分都藏在了 java.sql 这个包里，所有的枚举类，普通类以及接口，都在下方列了出来，其中，枚举和普通类是粗体，接口是正常字体。
java.sql.Array
java.sql.BatchUpdateException
java.sql.Blob
java.sql.CallableStatement
java.sql.Clob
java.sql.ClientinfoStatus
java.sql.Connection
java.sql.DataTruncation
java.sql.DatabaseMetaData
java.sql.Date
java.sql.Driver
java.sql.DriverAction
java.sql.DriverManager
java.sql.DriverPropertyInfo
java.sql.JDBCType
java.sql.NClob
java.sql.ParameterMetaData
java.sql.PreparedStatement
java.sql.PseudoColumnUsage
java.sql.Ref
java.sql.ResultSet
java.sql.ResultSetMetaData
java.sql.RowId
java.sql.RowIdLifeTime
java.sql.Savepoint
java.sql.SQLClientInfoException
java.sql.SQLData
java.sql.SQLDataException
java.sql.SQLException
java.sql.SQLFeatureNotSupportedException
java.sql.SQLInput
java.sql.SQLIntegrityConstraintViolationException
java.sql.SQLInvalidAuthorizationSpecException
java.sql.SQLNonTransientConnectionException
java.sql.SQLNonTransientException
java.sql.SQLOutput
java.sql.SQLPermission
java.sql.SQLSyntaxErrorException
java.sql.SQLTimeoutException
java.sql.SQLTransactionRollbackException
java.sql.SQLTransientConnectionException
java.sql.SQLTransientException
java.sql.SQLType
java.sql.SQLXML
java.sql.SQLWarning
java.sql.Statement
java.sql.Struct
java.sql.Time
java.sql.Timestamp
java.sql.Types
java.sql.Wrapper
下面这些类和接口是在 JDBC 4.2 API 中新增加或有过改动的，其中新增加的类和接口以粗体的形式表示
java.sql.BatchUpdateException
java.sql.CallableStatement
java.sql.Connection
java.sql.DatabaseMetaData</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第四章 JDBC API 概览</title>
      <link>/2017/jdbc4.2-spec-4/</link>
      <pubDate>Mon, 02 Jan 2017 13:46:42 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-4/</guid>
      <description>JDBC API 给 Java 程序提供了一种访问一个或者多个数据源的途径，在大多数情况下，数据源是关系型数据库，使用 SQL 语言来访问。但是，JDBC Driver 也可以实现为能够访问其它类型的数据源，比如说文件系统或面向对象的系统。 JDBC API 最主要的动机就是提供一种标准的 API ，让应用程序访问多种多样的数据源。
这一章介绍了 JDBC API 的一些关键概念，此外，也介绍 JDBC 程序的两种使用场景，分别是两层模型和三层模型，在不同的场景中，JDBC API 的功能是不一样的。
4.1 建立连接 JDBC API 定义了 Connection 接口来代表与某个数据源的一条连接。
典型情况下，JDBC 应用可以使用以下两种机制来与目标数据源建立连接
  DriverManager — 这个类从 JDBC API 1.0 版本开始就有了，当应用程序第一次尝试去连接一个数据源时，它需要指定一个url，DriverManager 将会自动加载所有它能在 CLASSPATH 下找到的 JDBC 驱动（任何 JDBC API 4.0 版本前的驱动，需要手动去加载）。
  DataSource — 这个接口在 JDBC 2.0 Optionnal Package API 中首次被引进，更推荐使用 DataSource， 因为它允许关于底层数据源的具体信息对于应用来说是透明的。需要设置 DataSource 对象的一些属性，这样才能让它代表某个数据源。当这个接口的 getConnection 方法被调用时，这个方法会返回一条与数据源建立好的连接。应用程序可以通过改变 DataSource 对象的属性，从而让它指向不同的数据源，无须改动应用代码；同时 DataSource 接口的具体实现类也可以在不改动应用程序代码的情况下，进行改变。</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第三章 新特性</title>
      <link>/2017/jdbc4.2-spec-3/</link>
      <pubDate>Mon, 02 Jan 2017 13:42:51 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-3/</guid>
      <description>JDBC API 4.2 规范在以下几个方面有所改动
3.1 增加对 REF CURSOR 的支持 有些数据库支持 REF CURSOR 数据类型，在调用存储过程后返回该类型的结果集。
3.2 支持大数量的更新 JDBC 当前的方法里返回一个更新数量时，返回的是一个 int，在某些场景下这会导致问题，因为数据集还在不停地增长。
3.3 增加 java.sql.DriverAction 接口 如果一个 driver 想要在它被 DriverManager 注销时得到通知，就要实现这个接口。
3.4 增加 java.sql.SQLType 接口 用来创建一个代表 SQL 类型的对象
3.5 增加 java.sql.JDBCType 枚举类 用来识别通用的 SQL 类型，目的是为了取代定义在 Types.java 类里的常量。
3.6 增加 Java Object 类型与 JDBC 类型的映射（附录表B-4） 增加 java.time.LocalDate 映射到 JDBC DATE
增加 java.time.LocalTime 映射到 JDBC TIME
增加 java.time.LocalDateTime 映射到 JDBC TIMESTAMP
增加 java.time.LocalOffsetTime 映射到 JDBC TIME_WITH_TIMEZONE</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第二章 目标</title>
      <link>/2017/jdbc4.2-spec-2/</link>
      <pubDate>Mon, 02 Jan 2017 13:40:18 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-2/</guid>
      <description>2.1 JDBC API 的历史 JDBC API 是一种成熟的技术，1997 年， 首次提出了 JDBC 规范，初始的版本中， JDBC API 仅提供一套针对数据库的调用级别接口。
从 2.0 版本和 2.1 版本开始，JDBC API 的功能得到增强，它能够支持更高级别的应用程序， 也提供一些高级特性，供开发应用服务器的开发者使用。
3.0 版本小范围地补充了一些遗漏的功能。
4.2 版本的目标主要有两个
 增强应用程序开发者对于 “简易开发” 的体验 增强企业应用级别的特性，提供一些工具集和 API 来更好地管理数据源  2.2 JDBC API 的目标 下面列出了 JDBC API 的目标和设计理念
2.2.1 与 Java EE 和 Java SE 平台无缝融合 JDBC API 是 Java 平台的一部分，JDBC 4.2 API 应该跟随着 Java SE 平台和 Java EE 平台的大方向走，对于 Java 语言的新特性和改进，都应该也体现在 JDBC API 4.2 的规范中。</description>
    </item>
    
    <item>
      <title>JDBC4.2规范-第一章 简介</title>
      <link>/2017/jdbc4.2-spec-1/</link>
      <pubDate>Mon, 02 Jan 2017 13:34:42 +0800</pubDate>
      
      <guid>/2017/jdbc4.2-spec-1/</guid>
      <description>1.1 JDBC API 简介 JDBC API 为 Java 语言提供了一种访问关系型数据库的方法。有了 JDBC API，应用程序便可以通过它来执行 SQL 语句、获取执行结果，以及对底层的数据库进行写入。JDBC API 也可以用来和多个数据源交互，这些数据源以分布式的形式部署。
JDBC API 基于 X/Open SQL CLI 标准，ODBC 也以它为标准。对于 X/Open SQL CLI 标准所定义的一些抽象概念，JDBC API 提供了对应的概念的映射，并且非常易用以及容易理解。
自从 1997 年 JDBC API 首次被提出以后， 它的被接受程度越来越大，并且也出现了大量的对于 JDBC API 规范的实现，这都归功于 JDBC API 本身的灵活性。
1.2 平台 JDBC API 是 Java 语言的一部分，JDBC API 分为两个 package，分别是 java.sql 和 javax.sql。在 Java SE 平台和 Java EE 平台都存在这两个 package。
1.3 目标读者  需要实现相应的驱动的数据库厂商 需要在 JDBC 驱动上建立多层服务的应用服务器厂商 需要基于 JDBC API 来开发应用的厂商  本文档也适用于任何在 JDBC API 上层开发应用的开发者</description>
    </item>
    
    <item>
      <title>Netty4.x Internal Logger 机制</title>
      <link>/2016/netty-internal-logger/</link>
      <pubDate>Sat, 25 Jun 2016 22:43:12 +0800</pubDate>
      
      <guid>/2016/netty-internal-logger/</guid>
      <description>简介 Netty是一个简化Java NIO编程的网络框架。就像人要吃饭一样，框架也要打日志。 Netty不像大多数框架，默认支持某一种日志实现。相反，Netty本身实现了一套日志机制，但这套日志机制并不会真正去打日志。相反，Netty自身的日志机制更像一个日志包装层。
日志框架检测顺序 Netty在启动的时候，会自动去检测当前Java进程的classpath下是否已经有其它的日志框架。 检查的顺序是： 先检查是否有slf4j，如果没有则检查是否有Log4j，如果上面两个都没有，则默认使用JDK自带的日志框架JDK Logging。 JDK的Logging就不用费事去检测了，直接拿来用了，因为它是JDK自带的。
 注意到虽然Netty支持Common Logging，但在Netty本文所用的4.10.Final版本的代码里，没有去检测Common Logging，即使有支持Common Logging的代码存在。
 日志框架检测细节 在Netty自身的代码里面，如果需要打日志，会通过以下代码来获得一个logger，以io.netty.bootstrap.Bootstrap这个类为例，读者可以翻开这个类瞧一瞧。
private static final InternalLogger logger = InternalLoggerFactory.getInstance(Bootstrap.class); 要知道Netty是怎么得到logger的，关键就在于这个InternalLoggerFactory类了，可以看出来，所有的logger都是通过这个工厂类产生的。 翻开InternalLoggerFactory类的代码，可以看到类中有一个静态初始化块
private static volatile InternalLoggerFactory defaultFactory; static { final String name = InternalLoggerFactory.class.getName(); InternalLoggerFactory f; try { f = new Slf4JLoggerFactory(true); f.newInstance(name).debug(&amp;#34;Using SLF4J as the default logging framework&amp;#34;); defaultFactory = f; } catch (Throwable t1) { try { f = new Log4JLoggerFactory(); f.newInstance(name).debug(&amp;#34;Using Log4J as the default logging framework&amp;#34;); } catch (Throwable t2) { f = new JdkLoggerFactory(); f.</description>
    </item>
    
    <item>
      <title>Java 实现生命周期管理机制</title>
      <link>/2016/java-lifecycle/</link>
      <pubDate>Wed, 13 Jan 2016 18:22:03 +0800</pubDate>
      
      <guid>/2016/java-lifecycle/</guid>
      <description>前言 最近一直在研究某个国产开源的MySQL数据库中间件，拉下其最新版的代码到eclipse后，启动起来，然后做各种测试和代码追踪；用完想要关闭它时，拉出它的STOP类想要运行时，发现这个类里赫然只写以下几行代码，于是我感觉瞬间受到了很多伤害。
public static void main(String[] args) { System.out.println(new Date() + &amp;#34;,server shutdown!&amp;#34;); } 这个中间件启动和运行的时候，开启了监听，启动着许多线程在跑着，并且有许多socket连接。但是并没有找到一个优雅的方式将其关闭。于是无奈之下，我只能去点eclipse的心碎小红点，强行停掉VM。
如果是一个架构良好，模块化清晰的软件，特别是Server类的软件，拥有一套生命周期管理机制是非常重要的。不仅可以管理各个模块的生命周期，也可以在启停整个软件的时候更优雅，不会漏掉任何资源。
生命周期机制简易实现 生命周期状态 一个模块的生命周期状态一般有以下几个：
 新生 -&amp;gt; 初始化中 -&amp;gt; 初始化完成 -&amp;gt; 启动中 -&amp;gt; 启动完成 -&amp;gt; 正在暂停 -&amp;gt; 已经暂停 -&amp;gt; 正在恢复 -&amp;gt; 已经恢复 -&amp;gt; 正在销毁 -&amp;gt; 已经销毁
 其中，任何一个状态之间的转化如果失败，那么就会进入另外一种状态：失败。
为此，可以用一个枚举类来枚举出这几个状态，如下所示：
public enum LifecycleState { NEW, //新生  INITIALIZING, INITIALIZED, //初始化  STARTING, STARTED, //启动  SUSPENDING, SUSPENDED, //暂停  RESUMING, RESUMED,//恢复  DESTROYING, DESTROYED,//销毁  FAILED;//失败  } 接口 生命周期中的各种行为规范，也需要一个接口来定义，如下所示:</description>
    </item>
    
    <item>
      <title>走进 Linux Rpm</title>
      <link>/2015/linux-rpm/</link>
      <pubDate>Mon, 03 Aug 2015 19:29:46 +0800</pubDate>
      
      <guid>/2015/linux-rpm/</guid>
      <description>包管理简介 什么是包(Packages)，为什么要管理它们 要回答这个问题，我们需要回到三个最基本的问题上面来：
 计算机 数据 程序  计算机需要获取数据和程序来做它应当做的事情，把数据和程序交给计算机，意味着把它们放进计算机的大容量存储里，现在，这又意味着放进硬盘里。数据和程序将会在硬盘里以文件的形式被存储。
而数据，数据不仅需要空间去存储它，更重要的是，它需要以程序能处理的格式存储。
最后，谈一谈程序，程序和数据一样，也需要一定的存储空间，但对于程序来说，以下几点更为重要：
 程序可能需要数据才能运行，这些数据必须格式正确、命名规范，并且存储在硬盘中某个合适的位置以便程序能够访问它。 程序可能需要配置文件，这些配置文件能够控制程序的行为，使程序表现出定制化的行为。 程序在硬盘上也需要工作空间，其次，程序也像数据一样，需要有一个合适的命名，并且存放在硬盘中合适的位置。 程序也有可能需要依赖其他的程序。 尽管一个程序的正常运行并不需要文档参与，但带有说明文档的程序，能以人类容易阅读的方式让使用者了解程序的使用方法。  现在想一想，当我们需要在电脑上安装软件时，我们可能采取的方式可能有以下两种：
  阅读程序的文档，把程序，配置文件，以及数据拷贝到你的电脑上，确保它们的命名规范，并且放在了硬盘上的合适位置，而且，硬盘有足够的空间来放下这些东西。接下来，按你的意愿修改一下配置文件，最后，运行程序。
  让电脑为你做这些事。
  如果你觉得第一种方式还OK啊，可以接受。但是你有没有想过你需要同时追踪多少个文件，在Linux系统中，一个程序有超过两万个文件是很正常的事情，有大量的文档需要你去阅读，大量的文件需要拷贝，还有配置。而且，当你想要更新软件版本的时候，你要怎么办？凡此种种，不一而足。
有些人会觉得第二种方式最简单啦：让电脑为你做这些事。RPM的出现，就是为了满足这些人的期待！
走进“包(Packages)” 计算机能像赶鸭子一样，很好地管理2万个以上的文件，这也是包管理软件擅长做的。不过，说了这么久，到底什么是包？
计算机眼中的包和我们日常生活中见到的包，其实是相似的，它们都能够把一些相关的东西放在同一个地方。在它们被使用前，它们都需要先被“打开”，在包上可以贴一个标签，以说明它里面装的是什么东西。
一般，包管理系统会把各种不同的文件，包括程序，数据，文档和配置信息，全部打包在一个特定格式的文件里，这个文件就叫一个包文件。以RPM为例，这个包文件叫做“package”，“.rpm文件”,或者直接就叫“RPM”，名字不同，但其实代表的是一样东西。一个包，包含了RPM安装所需要的所有东西。
一个rpm包通常包含下面这些类型的软件：
 一系列只有单个任务的程序集合，通常也叫作“应用(Application)”，比如说字符处理程序，或者是一门编程语言。 操作系统的一个特定部分，例如，操作系统启动时的初始化脚本，或一个特别的命令行SHELL，或者是一个支持web服务器的软件。  使用包的好处 使用包的一个最明显的好处是包是作为一个整体被管理的，如果需要移动该包，只需要移动整个包，而不用担心会漏掉某些文件，尽管这是一个最明显的好处，但是却不是最大的好处。
使用包的最大的好处是，包本身携带了它应该如何被安装的信息。不仅可以携带安装的步骤信息，也可以携带卸载所需要的步骤信息。
管理你的包吧，不然你会被它管理。 尽管包的使用已经降低了软件安装的复杂度，但它还是做不到不用你任何的参与就能做到安装，卸载。跟踪哪些包已经安装在你的系统上了，这是件很必要的事情，特别是当你所要安装的包依赖于其他包时。
包的管理也需要你动手 你会发现，你对包的管理很可能就是在做以下这些事：
 安装新的包。 更新包。 卸载包。  你需要总是做这些事情，也会很容易就无法对包的信息进行跟踪和掌握。你应该知道些什么有关于包的信息呢？
跟踪并管理包  很显然，你一定十分渴望看看在自己的操作系统上有哪些包已经安装了。 如果能获得你指定的某个包的信息，那就更好了。这些信息包括了从包开始安装的时间开始到一大串文件被安装结束。 能够通过多种方式获得包的信息也是激动人心的，技能找出一个包安装了什么文件，也能找出哪些包安装了某个特定的文件。 也应该能够查看一个包现在的安装方式和它之前的安装方式有何不同，误删文件是一件很平常的事，如果包管理器能够告诉你缺失了哪些文件，那么就能让你的软件正常运作起来。 配置文件包含的信息也让人很头疼，如果能够多注意这些配置文件，保证配置的改变不会丢失，那么，生活就会更愉悦了。  包管理，应该怎么做？ 前文为你描述了一个美好的愿景：包管理，能够让你更容易安装、更新和删除包；以多种方式查看包的信息；确保正确安装了包；甚至追踪配置文件的改动。但是，你要怎么做到这些呢？
前文也已经提过，最好的方式去做这些事就是让你的电脑帮你做。很多公司和组织已经开发了包管理系统。包管理系统主要有两种实现方式：
 一些包管理系统关注的是使用包的步骤。 另外一些包管理系统关注的是包所涉及的文件，并对这些文件进行修改追踪。  这两种实现方式有各自的优势，但也有各自的缺点。第一种方式，能更容易地安装新的包，但是删除旧包很困难，而且，几乎不可能得到任何关于已安装包的有意义的信息。
第二种方式得到已安装包的有用信息很容易，安装和删除包也比较容易。但这方式一个最不好的地方在于它无法在安装或删除包时执行一些特别的命令。
而实际上，没有一个包管理系统使用单独一种方式来实现，都是两种方式的组合实现，
RPM的设计目标 RPM的设计目标可以用一句话来概括:&amp;ldquo;something for everyone&amp;rdquo;，尽管RPM存在的主要原因是Red Hat公司为了更方便地在它们的linux发行版中安装几百个包，但这并不是RPM存在的唯一理由。我们可以看看Red Hat公司在设计RPM时的需求:</description>
    </item>
    
    <item>
      <title>Jedis的Sharded源代码分析</title>
      <link>/2015/jedis-sharded-explain/</link>
      <pubDate>Mon, 20 Apr 2015 18:53:03 +0800</pubDate>
      
      <guid>/2015/jedis-sharded-explain/</guid>
      <description>概述 Jedis是Redis官方推荐的Java客户端，更多Redis的客户端可以参考Redis官网客户端列表。当业务的数据量非常庞大时，需要考虑将数据存储到多个缓存节点上，如何定位数据应该存储的节点，一般用的是一致性哈希算法。Jedis在客户端角度实现了一致性哈希算法，对数据进行分片，存储到对应的不同的redis实例中。 Jedis对Sharded的实现主要是在ShardedJedis.java和ShardedJedisPool.java中。本文主要介绍ShardedJedis的实现，ShardedJedisPool是基于apache的common-pool2的对象池实现。
继承关系 ShardedJedis&amp;mdash;&amp;gt;BinaryShardedJedis&amp;mdash;&amp;gt;Sharded &amp;lt;Jedis, JedisShardInfo&amp;gt;
构造函数 查看其构造函数
public ShardedJedis(List&amp;lt;JedisShardInfo&amp;gt; shards, Hashing algo, Pattern keyTagPattern) { super(shards, algo, keyTagPattern); } 构造器参数解释：
  shards是一个JedisShardInfo的列表，一个JedisShardedInfo类代表一个数据分片的主体。
  algo是用来进行数据分片的算法
  keyTagPattern，自定义分片算法所依据的key的形式。例如，可以不针对整个key的字符串做哈希计算，而是类似对**thisisa{key}**中包含在大括号内的字符串进行哈希计算。
  JedisShardInfo是什么样的？
public class JedisShardInfo extends ShardInfo&amp;lt;Jedis&amp;gt; { public String toString() { return host + &amp;#34;:&amp;#34; + port + &amp;#34;*&amp;#34; + getWeight(); } private int connectionTimeout; private int soTimeout; private String host; private int port; private String password = null; private String name = null; // Default Redis DB  private int db = 0; public String getHost() { return host; } public int getPort() { return port; } public JedisShardInfo(String host) { super(Sharded.</description>
    </item>
    
    <item>
      <title>Jedis的JedisSentinelPool源代码分析</title>
      <link>/2015/jedis-sentinel-pool-explain/</link>
      <pubDate>Mon, 20 Apr 2015 18:50:36 +0800</pubDate>
      
      <guid>/2015/jedis-sentinel-pool-explain/</guid>
      <description>概述 Jedis是Redis官方推荐的Java客户端，更多Redis的客户端可以参考Redis官网客户端列表。Redis-Sentinel作为官方推荐的HA解决方案，Jedis也在客户端角度实现了对Sentinel的支持，主要实现在JedisSentinelPool.java这个类中，下文会分析这个类的实现。
属性 JedisSentinelPool类里有以下的属性：
//基于apache的commom-pool2的对象池配置 protected GenericObjectPoolConfig poolConfig; //超时时间，默认是2000 protected int timeout = Protocol.DEFAULT_TIMEOUT; //sentinel的密码 protected String password; //redis数据库的数目 protected int database = Protocol.DEFAULT_DATABASE; //master监听器，当master的地址发生改变时，会触发这些监听者 protected Set&amp;lt;MasterListener&amp;gt; masterListeners = new HashSet&amp;lt;MasterListener&amp;gt;(); protected Logger log = Logger.getLogger(getClass().getName()); //Jedis实例创建工厂 private volatile JedisFactory factory; //当前的master，HostAndPort是一个简单的包装了ip和port的模型类 private volatile HostAndPort currentHostMaster; 构造器 构造器的代码如下：
public JedisSentinelPool(String masterName, Set&amp;lt;String&amp;gt; sentinels, final GenericObjectPoolConfig poolConfig, int timeout, final String password, final int database) { this.poolConfig = poolConfig; this.timeout = timeout; this.</description>
    </item>
    
    <item>
      <title>redis sentinel 机制与用法</title>
      <link>/2015/redis-sentinel/</link>
      <pubDate>Fri, 17 Apr 2015 22:08:56 +0800</pubDate>
      
      <guid>/2015/redis-sentinel/</guid>
      <description>本文参考自《Redis Sentinel Documentation》
 概述 Redis-Sentinel是Redis官方推荐的高可用性(HA)解决方案，当用Redis做Master-slave的高可用方案时，假如master宕机了，Redis本身(包括它的很多客户端)都没有实现自动进行主备切换，而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。
它的主要功能有以下几点
 不时地监控redis是否按照预期良好地运行; 如果发现某个redis节点运行出现状况，能够通知另外一个进程(例如它的客户端); 能够进行自动切换。  当一个master节点不可用时，能够选举出master的多个slave(如果有超过一个slave的话)中的一个来作为新的master，其它的slave节点会将它所追随的master的地址改为被提升为master的slave的新地址。
Sentinel支持集群 很显然，只使用单个sentinel进程来监控redis集群是不可靠的，当sentinel进程宕掉后(sentinel本身也有单点问题，single-point-of-failure)整个集群系统将无法按照预期的方式运行。所以有必要将sentinel集群，这样有几个好处：
 即使有一些sentinel进程宕掉了，依然可以进行redis集群的主备切换； 如果只有一个sentinel进程，如果这个进程运行出错，或者是网络堵塞，那么将无法实现redis集群的主备切换（单点问题）; 如果有多个sentinel，redis的客户端可以随意地连接任意一个sentinel来获得关于redis集群中的信息。  Sentinel版本 Sentinel当前最新的稳定版本称为Sentinel 2(与之前的Sentinel 1区分开来）。随着redis2.8的安装包一起发行。安装完Redis2.8后，可以在**redis2.8/src/**里面找到Redis-sentinel的启动程序。
 强烈建议： 如果你使用的是redis2.6(sentinel版本为sentinel 1)，你最好应该使用redis2.8版本的sentinel 2，因为sentinel 1有很多的Bug，已经被官方弃用，所以强烈建议使用redis2.8以及sentinel 2。
 运行Sentinel 运行sentinel有两种方式：
 第一种  redis-sentinel /path/to/sentinel.conf  第二种  redis-server /path/to/sentinel.conf --sentinel 以上两种方式，都必须指定一个sentinel的配置文件sentinel.conf，如果不指定，将无法启动sentinel。sentinel默认监听26379端口，所以运行前必须确定该端口没有被别的进程占用。
Sentinel的配置 Redis源码包中包含了一个sentinel.conf文件作为sentinel的配置文件，配置文件自带了关于各个配置项的解释。典型的配置项如下所示：
sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 sentinel monitor resque 192.</description>
    </item>
    
    <item>
      <title>图与其Java实现</title>
      <link>/2015/java-graph/</link>
      <pubDate>Fri, 17 Apr 2015 18:07:51 +0800</pubDate>
      
      <guid>/2015/java-graph/</guid>
      <description>图的基本概念 图是什么，图是一种数据结构，一种非线性结构，所谓的非线性结构，浅显地理解的话，就是图的存储不是像链表这样的线性存储结构，而是由两个集合所组成的一种数据结构。
一个图中有两类东西，一种是结点，一种结点之间的连线。要用一种数据结构来表示的话，首先我们需要一个集合来存储所有的点，我们用V这个集合来表示（vertex），还需要另一个集合来存储所有的边，我们用E来表示(Edge)，那么一个图就可以表示为：
 G = (V，E）
 有的图的边是有方向的，有的是没有方向的。
（A,B）表示A结点与B结点之间无方向的边，&amp;lt;A,B&amp;gt;则表示方向为从A到B的一条边，当然，如果是&amp;lt;B,A&amp;gt;，则方向相反。因此从边的方向我们就可以把图分为有向图和无向图两种。
其它概念 一个图中的元素有很多，例如：
 完全图，邻接节点，结点的度，路径，权，路径长度，子图，连通图和强连通图，生成树，简单路径和回路&amp;hellip;
 本文只说说容易混淆的概念。
完全图，连通图，与强连通图 完全图可分为有向完全图和无向完全图两种，如果一个图的任意两个结点之间有且只有一条边，则称此图为无向完全图，若任意两个结点之间有且只有方向相反的两条边，则称为有向完全图。
那么连通图与完全图有什么区别呢？连通图是指在无向图中，若图中任意一对结点之间都有路径可达，则称这个无向图是连通图，而强连通图则是对应于有向图来说的，其特点与连通图是一样的。只不过是有向的，所以加了&amp;quot;强&amp;rdquo;。
连通图与完全图的区别就是，完全图要求任意两点之间有边，而连通图则是要求有路径。边和路径是有区别的。
邻接结点 一个结点的邻接节点，对于无向图来说，就是与这个结点相连的结点，至少有一个。
对于有向图来说，由于边是有方向的，所以一个结点的邻接节点是指以这个结点为开头，所指向的那些结点。
结点的度 度是针对结点来说的， 又分为出度和入度，看到“出度入度”，我们不难想到这是与边和边的方向有关的。 对于无向图来说，没有出度入度之分，一个结点的度就是经过这个结点的边的数目(或者是与这个结点相关联的边的数目)，对于有向图来说，出度就是指以这个结点为起始的边的条数（箭头向外），入度则是以这个点为终点的边的条数（箭头向内）。
出 = 箭头向外，入 = 箭头向内
权 权是指一条边所附带的数据信息，比如说一个结点到另一个结点的距离，或者花费的时间等等都可以用权来表示。带权的图也称为网格或网。
子图 跟一个集合有子集一样，图也有子图。可以类比理解。
存储结构 要存储一个图，我们知道图既有结点，又有边，对于有权图来说，每条边上还带有权值。常用的图的存储结构主要有以下二种：
 邻接矩阵 邻接表  邻接矩阵 我们知道，要表示结点，我们可以用一个一维数组来表示，然而对于结点和结点之间的关系，则无法简单地用一维数组来表示了，我们可以用二维数组来表示，也就是一个矩阵形式的表示方法。
我们假设A是这个二维数组，那么A中的一个元素aij不仅体现出了结点vi和结点vj的关系，而且aij的值正可以表示权值的大小。
以下是一个无向图的邻接矩阵表示示例：
从上图我们可以看到，无向图的邻接矩阵是对称矩阵，也一定是对称矩阵。且其左上角到右下角的对角线上值为零（对角线上表示的是相同的结点）
有向图的邻接矩阵是怎样的呢？
邻接表 我们知道，图的邻接矩阵存储方法用的是一个n*n的矩阵，当这个矩阵是稠密的矩阵（比如说当图是完全图的时候），那么当然选择用邻接矩阵存储方法。 可是如果这个矩阵是一个稀疏的矩阵呢，这个时候邻接表存储结构就是一种更节省空间的存储结构了。 对于上文中的无向图，我们可以用邻接表来表示，如下：
每一个结点后面所接的结点都是它的邻接结点。
邻接矩阵与邻接表的比较 当图中结点数目较小且边较多时，采用邻接矩阵效率更高。 当节点数目远大且边的数目远小于相同结点的完全图的边数时，采用邻接表存储结构更有效率。
邻接矩阵的Java实现 邻接矩阵模型类 邻接矩阵模型类的类名为AMWGraph.java，能够通过该类构造一个邻接矩阵表示的图，且提供插入结点，插入边，取得某一结点的第一个邻接结点和下一个邻接结点。
import java.util.ArrayList; import java.util.LinkedList; /** * @description 邻接矩阵模型类 * @author beanlam * @time 2015.</description>
    </item>
    
  </channel>
</rss>